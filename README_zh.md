<p align="center"> <img src="resources/title.png" style="width: 85%;" id="title-icon">       </p>

<p align="center">
   ğŸŒ <a href="https://cevalbenchmark.com/" target="_blank">ç½‘ç«™</a> â€¢ ğŸ¤— <a href="https://huggingface.co/datasets/ceval/ceval-exam" target="_blank">Hugging Face</a> â€¢ â¬ <a href="#æ•°æ®" target="_blank">æ•°æ®</a> â€¢ ğŸ“ƒ <a href="https://arxiv.org/abs/2305.08322"" target="_blank">è®ºæ–‡</a> ğŸ“– <a href="resources/tutorial.md" target="_blank">æ•™ç¨‹</a> <br> <a href="https://github.com/SJTU-LIT/ceval/blob/main/README.md">English | <a href="https://github.com/SJTU-LIT/ceval/blob/main/README_zh.md">ä¸­æ–‡</a>
</p>



C-Evalæ˜¯å…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„ä¼°å¥—ä»¶ï¼Œæ¶µç›–äº†52ä¸ªä¸åŒå­¦ç§‘çš„13948ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œåˆ†ä¸ºå››ä¸ªéš¾åº¦çº§åˆ«ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„[ç½‘ç«™](https://cevalbenchmark.com/)æˆ–æŸ¥çœ‹æˆ‘ä»¬çš„[è®ºæ–‡](https://arxiv.org/abs/2305.08322)ã€‚

æˆ‘ä»¬å¸Œæœ›C-Evalèƒ½å¤Ÿå¸®åŠ©å¼€å‘äººå‘˜è·Ÿè¸ªæ¨¡å‹å¼€å‘çš„è¿›å±•ï¼Œä»¥åŠåˆ†æå¼€å‘ä¸­æ¨¡å‹çš„ä¼˜ç‚¹å’Œå¼±ç‚¹ã€‚

ğŸ“ [ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦åšC-Evalï¼Ÿå…·ä½“æ€ä¹ˆåšçš„ï¼Ÿ](https://yaofu.notion.site/C-Eval-6b79edd91b454e3d8ea41c59ea2af873)

<img src="resources/overview.png" style="zoom: 80%;" >



## ç›®å½•

- [æ’è¡Œæ¦œ](#æ’è¡Œæ¦œ)
- [C-Eval Hard æ’è¡Œæ¦œ](#c-eval-hard-æ’è¡Œæ¦œ)
- [éªŒè¯é›†ç»“æœ](#éªŒè¯é›†ç»“æœ)
- [æ•°æ®](#æ•°æ®)
- [å¦‚ä½•åœ¨C-Evalä¸Šæµ‹è¯•](#å¦‚ä½•åœ¨c-evalä¸Šæµ‹è¯•)
- [å¦‚ä½•æäº¤](#å¦‚ä½•æäº¤)
- [TODO](#todo)
- [Licenses](#licenses)
- [å¼•ç”¨](#å¼•ç”¨)



## æ’è¡Œæ¦œ

ä¸‹é¢åˆ—å‡ºäº†æˆ‘ä»¬åœ¨åˆå§‹ç‰ˆæœ¬ä¸­è¿›è¡Œè¯„ä¼°çš„æ¨¡å‹çš„zero-shotå’Œfive-shotå‡†ç¡®ç‡ï¼Œè¯·è®¿é—®æˆ‘ä»¬å®˜æ–¹[æ’è¡Œæ¦œ](https://cevalbenchmark.com/static/leaderboard_zh.html)äº†è§£æœ€æ–°æ¨¡å‹åŠå…¶åœ¨æ¯ä¸ªå­¦ç§‘ä¸­çš„è¯¦ç»†ç»“æœã€‚æˆ‘ä»¬æ³¨æ„åˆ°å¯¹äºè®¸å¤šæŒ‡ä»¤å¾®è°ƒä¹‹åçš„æ¨¡å‹æ¥è¯´ï¼Œzero-shotç»“æœå¥½äºfew-shotã€‚

#### Zero-shot
| Model               | STEM | Social Science | Humanities | Other | Average |
| ------------------- | :--: | :------------: | :--------: | :---: | :-----: |
| GPT-4               | 65.2 |      74.7      |    62.5    | 64.7  |  66.4   |
| ChatGPT             | 49.0 |      58.0      |    48.8    | 50.4  |  51.0   |
| Claude-v1.3         | 48.5 |      58.6      |    47.3    | 50.1  |  50.5   |
| Bloomz-mt-176B           | 39.1 |      53.0      |    47.7    | 42.7  |  44.3   |
| GLM-130B            | 36.7 |      55.8      |    47.7    | 43.0  |  44.0   |
| Claude-instant-v1.0 | 38.6 |      47.6      |    39.5    | 39.0  |  40.6   |
| ChatGLM-6B          | 33.3 |      48.3      |    41.3    | 38.0  |  38.9   |
| LLaMA-65B           | 32.6 |      41.2      |    34.1    | 33.0  |  34.7   |
| MOSS                | 31.6 |      37.0      |    33.4    | 32.1  |  33.1   |
| Chinese-Alpaca-13B  | 27.4 |      39.2      |    32.5    | 28.0  |  30.9   |
| Chinese-LLaMA-13B   | 28.8 |      32.9      |    29.7    | 28.0  |  29.6   |

#### Five-shot
| Model               | STEM | Social Science | Humanities | Other | Average |
| ------------------- | :--: | :------------: | :--------: | :---: | :-----: |
| GPT-4               | 67.1 |      77.6      |    64.5    | 67.8  |  68.7   |
| ChatGPT             | 52.9 |      61.8      |    50.9    | 53.6  |  54.4   |
| Claude-v1.3         | 51.9 |      61.7      |    52.1    | 53.7  |  54.2   |
| Claude-instant-v1.0 | 43.1 |      53.8      |    44.2    | 45.4  |  45.9   |
| GLM-130B            | 34.8 |      48.7      |    43.3    | 39.8  |  40.3   |
| Bloomz-mt-176B      | 35.3 |      45.1      |    40.5    | 38.5  |  39.0   |
| LLaMA-65B           | 37.8 |      45.6      |    36.1    | 37.1  |  38.8   |
| ChatGLM-6B          | 30.4 |      39.6      |    37.4    | 34.5  |  34.5   |
| Chinese LLaMA-13B   | 31.6 |      37.2      |    33.6    | 32.8  |  33.3   |
| MOSS                | 28.6 |      36.8      |    31.0    | 30.3  |  31.1   |
| Chinese Alpaca-13B  | 26.0 |      27.2      |    27.8    | 26.4  |  26.7   |



## C-Eval Hard æ’è¡Œæ¦œ

æˆ‘ä»¬é€‰å–äº†C-Evalä¸­å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦ã€ç‰©ç†å’ŒåŒ–å­¦ç§‘ç›®ç»„æˆC-Eval Hardï¼ŒåŒ…æ‹¬ï¼šé«˜ç­‰æ•°å­¦ã€ç¦»æ•£æ•°å­¦ã€æ¦‚ç‡ç»Ÿè®¡ã€å¤§å­¦åŒ–å­¦ã€å¤§å­¦ç‰©ç†ã€é«˜ä¸­æ•°å­¦ã€é«˜ä¸­ç‰©ç†ã€é«˜ä¸­åŒ–å­¦å…«ä¸ªç§‘ç›®ã€‚è¿™äº›ç§‘ç›®åŒ…å«äº†å¤æ‚çš„LaTexå…¬å¼ï¼Œéœ€è¦éå‡¡çš„æ¨ç†èƒ½åŠ›æ‰èƒ½è§£å†³ã€‚ä»¥ä¸‹æ˜¯0-shotå’Œ5-shotçš„å‡†ç¡®ç‡ã€‚

| Model               | Zero-shot | Five-shot |
| ------------------- | :-------: | :-------: |
| GPT-4               |   53.3    |   54.9    |
| Claude-v1.3         |   37.6    |   39.0    |
| ChatGPT             |   36.7    |   41.4    |
| Claude-instant-v1.0 |   32.1    |   35.5    |
| Bloomz-mt           |   30.8    |   30.4    |
| GLM-130B            |   30.7    |   30.3    |
| LLaMA-65B           |   29.8    |   31.7    |
| ChatGLM-6B          |   29.2    |   23.1    |
| MOSS                |   28.4    |   24.0    |
| Chinese-LLaMA-13B   |   27.5    |   27.3    |
| Chinese-Alpaca-13B  |   24.4    |   27.1    |



## éªŒè¯é›†ç»“æœ

å› ä¸ºæˆ‘ä»¬ä¸ä¼šå…¬å¼€å‘å¸ƒæµ‹è¯•æ•°æ®é›†çš„æ ‡ç­¾ï¼Œæ‰€ä»¥æˆ‘ä»¬æä¾›éªŒè¯é›†çš„å¹³å‡å‡†ç¡®ç‡ä½œä¸ºå‚è€ƒã€‚éªŒè¯é›†æ€»å…±æœ‰1346ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬åœ¨ä¸‹è¡¨ä¸­æä¾›åœ¨æ‰€æœ‰ç§‘ç›®ä¸Šçš„0-shotå’Œ5-shotå¹³å‡å‡†ç¡®ç‡ã€‚Valé›†çš„å¹³å‡å‡†ç¡®ç‡ä¸[æ’è¡Œæ¦œ](#æ’è¡Œæ¦œ)ä¸­å‘ˆç°çš„å¹³å‡æµ‹è¯•å‡†ç¡®ç‡æ¯”è¾ƒæ¥è¿‘ã€‚

| Model               | Zero-shot | Five-shot |
| ------------------- | :-------: | :-------: |
| GPT-4               |   66.7    |   69.9    |
| Claude-v1.3         |   52.1    |   55.5    |
| ChatGPT             |   50.8    |   53.5    |
| Bloomz-mt           |   45.9    |   38.0    |
| GLM-130B            |   44.2    |   40.8    |
| Claude-instant-v1.0 |   43.2    |   47.4    |
| ChatGLM-6B          |   39.7    |   37.1    |
| LLaMA-65B           |   38.6    |   39.8    |
| MOSS                |   35.1    |   28.9    |
| Chinese-Alpaca-13B  |   32.0    |   27.2    |
| Chinese-LLaMA-13B   |   29.4    |   33.1    |



## æ•°æ®

#### ä¸‹è½½

* æ–¹æ³•ä¸€ï¼šä¸‹è½½zipå‹ç¼©æ–‡ä»¶ï¼ˆä½ ä¹Ÿå¯ä»¥ç›´æ¥ç”¨æµè§ˆå™¨æ‰“å¼€ä¸‹é¢çš„é“¾æ¥ï¼‰ï¼š
  ```
  wget https://huggingface.co/datasets/ceval/ceval-exam/blob/main/ceval-exam.zip
  ```
  ç„¶åå¯ä»¥ä½¿ç”¨ pandasåŠ è½½æ•°æ®ï¼š

  ```
  import os
  import pandas as pd
  
  File_Dir="ceval-exam"
  test_df=pd.read_csv(os.path.join(File_Dir,"test","computer_network_test.csv"))
  ```

* æ–¹æ³•äºŒï¼šä½¿ç”¨[Hugging Face datasets](https://huggingface.co/datasets/ceval/ceval-exam)ç›´æ¥åŠ è½½æ•°æ®é›†ã€‚ç¤ºä¾‹å¦‚ä¸‹ï¼š

  ```python
  from datasets import load_dataset
  dataset=load_dataset(r"ceval/ceval-exam",name="computer_network")
  
  print(dataset['val'][0])
  # {'id': 0, 'question': 'ä½¿ç”¨ä½å¡«å……æ–¹æ³•ï¼Œä»¥01111110ä¸ºä½é¦–flagï¼Œæ•°æ®ä¸º011011111111111111110010ï¼Œæ±‚é—®ä¼ é€æ—¶è¦æ·»åŠ å‡ ä¸ª0____', 'A': '1', 'B': '2', 'C': '3', 'D': '4', 'answer': 'C', 'explanation': ''}
  ```

#### è¯´æ˜
ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨ï¼Œæˆ‘ä»¬å·²ç»æ•´ç†å‡ºäº†ä¸ 52 ä¸ªç§‘ç›®å¯¹åº”çš„å­¦ç§‘åç§°å¤„ç†ç¨‹åºä»¥åŠå®ƒä»¬çš„ä¸­è‹±æ–‡åç§°ã€‚å…·ä½“ç»†èŠ‚è¯·æŸ¥çœ‹ [subject_mapping.json](https://github.com/SJTU-LIT/ceval/blob/main/subject_mapping.json)ã€‚æ ¼å¼å¦‚ä¸‹ï¼š

```
{
	"computer_network": [
		"Computer Network",
		"è®¡ç®—æœºç½‘ç»œ",
		"STEM"
	],
	...
	"filename":[
	"è‹±æ–‡åç§°",
	"ä¸­æ–‡åç§°"
	"ç±»åˆ«(STEM,Social Science,Humanities,Otherå››é€‰ä¸€)"
	]
}
```

æ¯ä¸ªç§‘ç›®ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šdevã€val å’Œ testã€‚æ¯ä¸ªç§‘ç›®çš„ dev é›†åŒ…å«äº”ä¸ªç¤ºèŒƒå®ä¾‹ä»¥åŠä¸º few-shot è¯„ä¼°æä¾›çš„è§£é‡Šã€‚val é›†æ—¨åœ¨ç”¨äºè¶…å‚æ•°è°ƒæ•´ã€‚è€Œ test é›†åˆ™ç”¨äºæ¨¡å‹è¯„ä¼°ã€‚test é›†ä¸Šçš„æ ‡ç­¾ä¸ä¼šå…¬å¼€ï¼Œéœ€è¦ç”¨æˆ·æäº¤å…¶ç»“æœæ‰èƒ½è‡ªåŠ¨è·å¾—æµ‹è¯•å‡†ç¡®æ€§ã€‚[å¦‚ä½•æäº¤ï¼Ÿ](#å¦‚ä½•æäº¤) 

ä¸‹é¢æ˜¯é«˜ä¸­åŒ–å­¦çš„ç¤ºä¾‹ï¼š

```
id: 1
question: 25 Â°Cæ—¶ï¼Œå°†pH=2çš„å¼ºé…¸æº¶æ¶²ä¸pH=13çš„å¼ºç¢±æº¶æ¶²æ··åˆï¼Œæ‰€å¾—æ··åˆæ¶²çš„pH=11ï¼Œåˆ™å¼ºé…¸æº¶æ¶²ä¸å¼ºç¢±æº¶æ¶² çš„ä½“ç§¯æ¯”æ˜¯(å¿½ç•¥æ··åˆåæº¶æ¶²çš„ä½“ç§¯å˜åŒ–)____
A: 11:1
B: 9:1
C: 1:11
D: 1:9
answer: B
explanation: 
1. pH=13çš„å¼ºç¢±æº¶æ¶²ä¸­c(OH-)=0.1mol/L, pH=2çš„å¼ºé…¸æº¶æ¶²ä¸­c(H+)=0.01mol/Lï¼Œé…¸ç¢±æ··åˆåpH=11ï¼Œå³c(OH-)=0.001mol/Lã€‚
2. è®¾å¼ºé…¸å’Œå¼ºç¢±æº¶æ¶²çš„ä½“ç§¯åˆ†åˆ«ä¸ºxå’Œyï¼Œåˆ™ï¼šc(OH-)=(0.1y-0.01x)/(x+y)=0.001ï¼Œè§£å¾—x:y=9:1ã€‚
```



## å¦‚ä½•åœ¨C-Evalä¸Šæµ‹è¯•

é€šå¸¸ä½ å¯ä»¥ç›´æ¥ä»æ¨¡å‹çš„ç”Ÿæˆä¸­ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–å‡ºç­”æ¡ˆé€‰é¡¹ï¼ˆA,B,C,D)ã€‚åœ¨å°‘æ ·æœ¬æµ‹è¯•ä¸­ï¼Œæ¨¡å‹é€šå¸¸ä¼šéµå¾ªå°‘æ ·æœ¬ç»™å‡ºçš„å›ºå®šæ ¼å¼ï¼Œæ‰€ä»¥æå–ç­”æ¡ˆå¾ˆç®€å•ã€‚ç„¶è€Œæœ‰æ—¶å€™ï¼Œç‰¹åˆ«æ˜¯é›¶æ ·æœ¬æµ‹è¯•å’Œé¢å¯¹æ²¡æœ‰åšè¿‡æŒ‡ä»¤å¾®è°ƒçš„æ¨¡å‹æ—¶ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•å¾ˆå¥½çš„ç†è§£æŒ‡ä»¤ï¼Œç”šè‡³æœ‰æ—¶ä¸ä¼šå›ç­”é—®é¢˜ã€‚è¿™ç§æƒ…å†µä¸‹æˆ‘ä»¬æ¨èç›´æ¥è®¡ç®—ä¸‹ä¸€ä¸ªé¢„æµ‹tokenç­‰äº"A", "B", "C", "D"çš„æ¦‚ç‡ï¼Œç„¶åä»¥æ¦‚ç‡æœ€å¤§çš„é€‰é¡¹ä½œä¸ºç­”æ¡ˆ -- è¿™æ˜¯ä¸€ç§å—é™è§£ç ç”Ÿæˆçš„æ–¹æ³•ï¼ŒMMLUçš„[å®˜æ–¹æµ‹è¯•ä»£ç ](https://github.com/hendrycks/test/blob/4450500f923c49f1fb1dd3d99108a0bd9717b660/evaluate.py#L88)ä¸­æ˜¯ä½¿ç”¨äº†è¿™ç§æ–¹æ³•è¿›è¡Œæµ‹è¯•ã€‚æ³¨æ„è¿™ç§æ¦‚ç‡æ–¹æ³•å¯¹æ€ç»´é“¾çš„æµ‹è¯•ä¸é€‚ç”¨ã€‚[æ›´åŠ è¯¦ç»†çš„è¯„æµ‹æ•™ç¨‹](resources/tutorial.md)ã€‚

åœ¨æˆ‘ä»¬æœ€åˆå‘å¸ƒæ—¶ï¼Œæˆ‘ä»¬è‡ªå·±ç”¨äº†ä»¥ä¸‹promptè¿›è¡Œæµ‹è¯•ï¼š
#### ä»…é¢„æµ‹ç­”æ¡ˆçš„prompt
```
ä»¥ä¸‹æ˜¯ä¸­å›½å…³äº{ç§‘ç›®}è€ƒè¯•çš„å•é¡¹é€‰æ‹©é¢˜ï¼Œè¯·é€‰å‡ºå…¶ä¸­çš„æ­£ç¡®ç­”æ¡ˆã€‚

{é¢˜ç›®1}
A. {é€‰é¡¹A}
B. {é€‰é¡¹B}
C. {é€‰é¡¹C}
D. {é€‰é¡¹D}
ç­”æ¡ˆï¼šA

[k-shot demo, note that k is 0 in the zero-shot case]

{æµ‹è¯•é¢˜ç›®}
A. {é€‰é¡¹A}
B. {é€‰é¡¹B}
C. {é€‰é¡¹C}
D. {é€‰é¡¹D}
ç­”æ¡ˆï¼š
```

#### æ€ç»´é“¾prompt
```
ä»¥ä¸‹æ˜¯ä¸­å›½å…³äº{ç§‘ç›®}è€ƒè¯•çš„å•é¡¹é€‰æ‹©é¢˜ï¼Œè¯·é€‰å‡ºå…¶ä¸­çš„æ­£ç¡®ç­”æ¡ˆã€‚

{é¢˜ç›®1}
A. {é€‰é¡¹A}
B. {é€‰é¡¹B}
C. {é€‰é¡¹C}
D. {é€‰é¡¹D}
ç­”æ¡ˆï¼šè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ€è€ƒï¼Œ
1. {è§£æè¿‡ç¨‹æ­¥éª¤1}
2. {è§£æè¿‡ç¨‹æ­¥éª¤2}
3. {è§£æè¿‡ç¨‹æ­¥éª¤3}
æ‰€ä»¥ç­”æ¡ˆæ˜¯Aã€‚

[k-shot demo, note that k is 0 in the zero-shot case]

{æµ‹è¯•é¢˜ç›®}
A. {é€‰é¡¹A}
B. {é€‰é¡¹B}
C. {é€‰é¡¹C}
D. {é€‰é¡¹D}
ç­”æ¡ˆï¼šè®©æˆ‘ä»¬ä¸€æ­¥ä¸€æ­¥æ€è€ƒï¼Œ
1. 
```



## å¦‚ä½•æäº¤

æ‚¨é¦–å…ˆéœ€è¦å‡†å¤‡ä¸€ä¸ª UTF-8 ç¼–ç çš„ JSON æ–‡ä»¶ï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç¼–å†™ã€‚è¯¦æƒ…è¯·å‚è€ƒ[submission_example.json](https://github.com/SJTU-LIT/ceval/blob/main/submission_example.json)ã€‚

```
## æ¯ä¸ªå­¦ç§‘å†…éƒ¨çš„é”®åæ˜¯æ•°æ®é›†ä¸­çš„"id"å­—æ®µ
{
    "chinese_language_and_literature": {
        "0": "A",
        "1": "B",
        "2": "B",
        ...
    },
    
    "å­¦ç§‘åç§°":{
    "0":"ç­”æ¡ˆ1",
    "1":"ç­”æ¡ˆ2",
    ...
    }
    ....
}
```

ç„¶åä½ å¯ä»¥å°†å‡†å¤‡å¥½çš„JSONæ–‡ä»¶æäº¤åˆ°[è¿™é‡Œ](https://cevalbenchmark.com/static/user_interface_zh.html)ï¼Œ**è¯·æ³¨æ„ï¼Œä½ éœ€è¦å…ˆç™»å½•æ‰èƒ½è®¿é—®æäº¤é¡µé¢**ã€‚



## TODO

- [x] æ·»åŠ zero-shotç»“æœ
- [ ] é›†æˆåˆ°openai eval



## Licenses

[![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)

æœ¬é¡¹ç›®éµå¾ª [MIT License](https://lbesson.mit-license.org/).

[![CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](http://creativecommons.org/licenses/by-nc-sa/4.0/)

C-Evalæ•°æ®é›†éµå¾ª [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).



## å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº†æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ã€‚

```
@article{huang2023ceval,
title={C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models}, 
author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and Fu, Yao and Sun, Maosong and He, Junxian},
journal={arXiv preprint arXiv:2305.08322},
year={2023}
}
```

