<p align="center"> <img src="resources/title.png" style="width: 85%;" id="title-icon">       </p>

<p align="center">
   ğŸŒ <a href="https://cevalbenchmark.com/" target="_blank">ç½‘ç«™</a> â€¢ ğŸ¤— <a href="https://huggingface.co/datasets/ceval/ceval-exam" target="_blank">Hugging Face</a> â€¢ â¬ <a href="#æ•°æ®" target="_blank">æ•°æ®</a> â€¢ ğŸ“ƒ <a href="https://arxiv.org/abs/2305.08322"" target="_blank">è®ºæ–‡</a> <br> <a href="https://github.com/SJTU-LIT/ceval/blob/main/README.md">English|<a href="https://github.com/SJTU-LIT/ceval/blob/main/README_zh.md">ä¸­æ–‡</a>
</p>



C-Eval æ˜¯å…¨é¢çš„ä¸­æ–‡åŸºç¡€æ¨¡å‹è¯„ä¼°å¥—ä»¶ï¼Œæ¶µç›–äº†52ä¸ªä¸åŒå­¦ç§‘çš„13948ä¸ªå¤šé¡¹é€‰æ‹©é¢˜ï¼Œåˆ†ä¸ºå››ä¸ªéš¾åº¦çº§åˆ«ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚æ›´å¤šè¯¦æƒ…ï¼Œè¯·è®¿é—®æˆ‘ä»¬çš„[ç½‘ç«™](https://cevalbenchmark.com/)æˆ–æŸ¥çœ‹æˆ‘ä»¬çš„[è®ºæ–‡](https://arxiv.org/abs/2305.08322)ã€‚

<img src="resources/overview.png" style="zoom: 80%;" >



## ç›®å½•

* [æ’è¡Œæ¦œ](#æ’è¡Œæ¦œ)
* [C-Eval Hard æ’è¡Œæ¦œ](#c-eval-hard-æ’è¡Œæ¦œ)
* [éªŒè¯é›†ç»“æœ](#éªŒè¯é›†ç»“æœ)
* [æ•°æ®](#æ•°æ®)
* [å¦‚ä½•æäº¤](#å¦‚ä½•æäº¤)
* [Licenses](#licenses)
* [å¼•ç”¨](#å¼•ç”¨)



## æ’è¡Œæ¦œ

ä¸‹é¢åˆ—å‡ºäº†æˆ‘ä»¬åœ¨åˆå§‹ç‰ˆæœ¬ä¸­è¿›è¡Œè¯„ä¼°çš„æ¨¡å‹ï¼Œè¯·è®¿é—®æˆ‘ä»¬å®˜æ–¹[æ’è¡Œæ¦œ](https://cevalbenchmark.com/static/leaderboard_zh.html)äº†è§£æœ€æ–°æ¨¡å‹åŠå…¶åœ¨æ¯ä¸ªå­¦ç§‘ä¸­çš„è¯¦ç»†ç»“æœã€‚

| Model               | STEM | Social Science | Humanities | Other | Average |
| ------------------- | :--: | :------------: | :--------: | :---: | :-----: |
| GPT-4               | 67.1 |      77.6      |    64.5    | 67.8  |  68.7   |
| ChatGPT             | 52.9 |      61.8      |    50.9    | 53.6  |  54.4   |
| Claude-v1.3         | 51.9 |      61.7      |    52.1    | 53.7  |  54.2   |
| MiniMax             | 40.6 |      60.3      |    56.6    | 46.6  |  49.0   |
| Claude-instant-v1.0 | 43.1 |      53.8      |    44.2    | 45.4  |  45.9   |
| GLM-130B            | 34.8 |      48.7      |    43.3    | 39.8  |  40.3   |
| Bloomz-mt           | 35.3 |      45.1      |    40.5    | 38.5  |  39.0   |
| LLaMA-65B           | 37.8 |      45.6      |    36.1    | 37.1  |  38.8   |
| ChatGLM-6B          | 30.4 |      39.6      |    37.4    | 34.5  |  34.5   |
| Chinese LLaMA-13B   | 31.6 |      37.2      |    33.6    | 32.8  |  33.3   |
| MOSS                | 28.6 |      36.8      |    31.0    | 30.3  |  31.1   |
| Chinese Alpaca-13B  | 26.0 |      27.2      |    27.8    | 26.4  |  26.7   |



## C-Eval Hard æ’è¡Œæ¦œ

æˆ‘ä»¬é€‰å–äº†C-Evalä¸­å…·æœ‰æŒ‘æˆ˜æ€§çš„æ•°å­¦ã€ç‰©ç†å’ŒåŒ–å­¦ç§‘ç›®ç»„æˆC-Eval Hardï¼ŒåŒ…æ‹¬ï¼šé«˜ç­‰æ•°å­¦ã€ç¦»æ•£æ•°å­¦ã€æ¦‚ç‡ç»Ÿè®¡ã€å¤§å­¦åŒ–å­¦ã€å¤§å­¦ç‰©ç†ã€é«˜ä¸­æ•°å­¦ã€é«˜ä¸­ç‰©ç†ã€é«˜ä¸­åŒ–å­¦å…«ä¸ªç§‘ç›®ã€‚è¿™äº›ç§‘ç›®åŒ…å«äº†å¤æ‚çš„LaTexå…¬å¼ï¼Œéœ€è¦éå‡¡çš„æ¨ç†èƒ½åŠ›æ‰èƒ½è§£å†³ã€‚

| Model               | Accuracy |
| ------------------- | :------: |
| GPT-4               |   54.9   |
| ChatGPT             |   41.4   |
| Claude-v1.3         |   39.0   |
| Claude-instant-v1.0 |   35.5   |
| LLaMA-65B           |   31.7   |
| Bloomz-mt           |   30.4   |
| GLM-130B            |   30.3   |
| Chinese LLaMA-13B   |   27.3   |
| MiniMax             |   27.3   |
| Chinese Alpaca-13B  |   27.1   |
| MOSS                |   24.0   |
| ChatGLM-6B          |   23.1   |



## éªŒè¯é›†ç»“æœ

å› ä¸ºæˆ‘ä»¬ä¸ä¼šå…¬å¼€å‘å¸ƒæµ‹è¯•æ•°æ®é›†çš„æ ‡ç­¾ï¼Œæ‰€ä»¥æˆ‘ä»¬æä¾›éªŒè¯é›†çš„å¹³å‡å‡†ç¡®ç‡ä½œä¸ºå‚è€ƒã€‚éªŒè¯é›†æ€»å…±æœ‰1346ä¸ªé—®é¢˜ï¼Œå¹³å‡æ¯ä¸ªç§‘ç›®ä¸åˆ°30ä¸ªé—®é¢˜ã€‚å› æ­¤ï¼Œç‰¹å®šç§‘ç›®çš„å‡†ç¡®ç‡ä¸å…·æœ‰å‚è€ƒä»·å€¼ã€‚æˆ‘ä»¬åœ¨ä¸‹è¡¨ä¸­æä¾›åœ¨æ‰€æœ‰ç§‘ç›®ä¸Šçš„å¹³å‡å‡†ç¡®ç‡ã€‚Valé›†çš„å¹³å‡å‡†ç¡®ç‡ä¸[æ’è¡Œæ¦œ](#æ’è¡Œæ¦œ)ä¸­å‘ˆç°çš„å¹³å‡æµ‹è¯•å‡†ç¡®ç‡éå¸¸æ¥è¿‘ã€‚

| Model               | Average |
| ------------------- | :-----: |
| GPT-4               |  69.9   |
| Claude-v1.3         |  55.5   |
| ChatGPT             |  53.5   |
| MiniMax             |  48.4   |
| Claude-instant-v1.0 |  47.4   |
| GLM-130B            |  40.8   |
| LLaMA-65B           |  39.8   |
| Bloomz-mt           |  38.0   |
| ChatGLM-6B          |  37.1   |
| Chinese-LLaMA-13B   |  33.1   |
| MOSS                |  28.9   |
| Chinese-Alpaca-13B  |  27.2   |



## æ•°æ®

#### ä¸‹è½½

* æ–¹æ³•ä¸€ï¼šä» [Onedrive](https://onedrive.live.com/download?cid=19737A21B01C55D4&resid=19737A21B01C55D4!983&authkey=AGch_tVH959ZJiw) ä¸‹è½½ï¼Œæ•°æ®ä»¥ csv æ ¼å¼å­˜å‚¨ï¼Œä½¿ç”¨ UTF-8 ç¼–ç ã€‚ç„¶åå¯ä»¥ä½¿ç”¨ pandasåŠ è½½æ•°æ®ï¼š

  ```
  import os
  import pandas as pd
  
  File_Dir="data"
  test_df=pd.read_csv(os.path.join(File_Dir,"test","advanced_mathematics_test.csv"))
  ```

* æ–¹æ³•äºŒï¼šä½¿ç”¨[huggingface datasets](https://huggingface.co/datasets/ceval/ceval-exam)ç›´æ¥åŠ è½½æ•°æ®é›†ã€‚ç¤ºä¾‹å¦‚ä¸‹ï¼š

  ```python
  from datasets import load_dataset
  dataset=load_dataset(r"ceval/ceval-exam",name="advanced_mathematics")
  ```

#### è¯´æ˜
ä¸ºäº†æ–¹ä¾¿ä½¿ç”¨ï¼Œæˆ‘ä»¬å·²ç»æ•´ç†å‡ºäº†ä¸ 52 ä¸ªç§‘ç›®å¯¹åº”çš„å­¦ç§‘åç§°å¤„ç†ç¨‹åºä»¥åŠå®ƒä»¬çš„ä¸­è‹±æ–‡åç§°ã€‚å…·ä½“ç»†èŠ‚è¯·æŸ¥çœ‹ [subject_mapping.json](https://github.com/SJTU-LIT/ceval/blob/main/subject_mapping.json)ã€‚æ ¼å¼å¦‚ä¸‹ï¼š

```
{
	"computer_network": [
		"Computer Network",
		"è®¡ç®—æœºç½‘ç»œ",
		"STEM"
	],
	...
	"filename":[
	"è‹±æ–‡åç§°",
	"ä¸­æ–‡åç§°"
	"ç±»åˆ«(STEM,Social Science,Humanities,Otherå››é€‰ä¸€)"
	]
}
```

æ¯ä¸ªç§‘ç›®ç”±ä¸‰ä¸ªéƒ¨åˆ†ç»„æˆï¼šdevã€val å’Œ testã€‚æ¯ä¸ªç§‘ç›®çš„ dev é›†åŒ…å«äº”ä¸ªç¤ºèŒƒå®ä¾‹ä»¥åŠä¸º few-shot è¯„ä¼°æä¾›çš„è§£é‡Šã€‚val é›†æ—¨åœ¨ç”¨äºè¶…å‚æ•°è°ƒæ•´ã€‚è€Œ test é›†åˆ™ç”¨äºæ¨¡å‹è¯„ä¼°ã€‚test é›†ä¸Šçš„æ ‡ç­¾ä¸ä¼šå…¬å¼€ï¼Œéœ€è¦ç”¨æˆ·æäº¤å…¶ç»“æœæ‰èƒ½è‡ªåŠ¨è·å¾—æµ‹è¯•å‡†ç¡®æ€§ã€‚[å¦‚ä½•æäº¤ï¼Ÿ](#å¦‚ä½•æäº¤) 

ä¸‹é¢æ˜¯è®¡ç®—æœºç½‘ç»œçš„ç¤ºä¾‹ï¼š

```
id: 1
question: æ»‘åŠ¨çª—å£çš„ä½œç”¨æ˜¯____ã€‚
A: æµé‡æ§åˆ¶
B: æ‹¥å¡æ§åˆ¶
C: è·¯ç”±æ§åˆ¶
D: å·®é”™æ§åˆ¶
answer: A
explantion: 1. æ»‘åŠ¨çª—å£æ˜¯ä¸€ç§æµé‡æ§åˆ¶æœºåˆ¶ï¼Œç”¨äºæ§åˆ¶å‘é€æ–¹å‘æ¥æ”¶æ–¹å‘é€æ•°æ®çš„é€Ÿç‡ï¼Œä»¥é¿å…æ¥æ”¶æ–¹æ— æ³•å¤„ç†è¿‡å¤šçš„æ•°æ®è€Œå¯¼è‡´æ•°æ®ä¸¢å¤±æˆ–æ‹¥å¡ã€‚
```



## å¦‚ä½•æäº¤

æ‚¨é¦–å…ˆéœ€è¦å‡†å¤‡ä¸€ä¸ª UTF-8 ç¼–ç çš„ JSON æ–‡ä»¶ï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼ç¼–å†™ã€‚è¯¦æƒ…è¯·å‚è€ƒ[submission_example.json](https://github.com/SJTU-LIT/ceval/blob/main/submission_example.json)ã€‚

```
## æ¯ä¸ªå­¦ç§‘å†…éƒ¨çš„é”®åæ˜¯æ•°æ®é›†ä¸­çš„"id"å­—æ®µ
{
    "chinese_language_and_literature": {
        "0": "A",
        "1": "B",
        "2": "B",
        ...
    },
    
    "å­¦ç§‘åç§°":{
    "0":"ç­”æ¡ˆ1",
    "1":"ç­”æ¡ˆ2",
    ...
    }
    ....
}
```

ç„¶åä½ å¯ä»¥å°†å‡†å¤‡å¥½çš„JSONæ–‡ä»¶æäº¤åˆ°[è¿™é‡Œ](https://cevalbenchmark.com/static/user_interface_zh.html)ï¼Œ**è¯·æ³¨æ„ï¼Œä½ éœ€è¦å…ˆç™»å½•æ‰èƒ½è®¿é—®æäº¤é¡µé¢**ã€‚



## Licenses

[![MIT license](https://img.shields.io/badge/License-MIT-blue.svg)](https://lbesson.mit-license.org/)

æœ¬é¡¹ç›®éµå¾ª [MIT License](https://lbesson.mit-license.org/).

[![CC BY-NC-SA 4.0](https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg)](http://creativecommons.org/licenses/by-nc-sa/4.0/)

C-Evalæ•°æ®é›†éµå¾ª [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/).



## å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº†æˆ‘ä»¬çš„æ•°æ®é›†ï¼Œè¯·å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ã€‚

```
@article{huang2023ceval,
title={C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models}, 
author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Lei, Jiayi and Qi, Fanchao and Fu, Yao and Sun, Maosong and He, Junxian},
journal={arXiv preprint arXiv:2305.08322},
year={2023}
}
```

